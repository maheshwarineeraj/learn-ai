{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d368a3d",
   "metadata": {},
   "source": [
    "# Advanced RAG with Cloud Vector Databases: Milvus Integration\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates advanced RAG implementation using cloud-based vector databases. You'll learn to build a personal knowledge base from blog content using Milvus (Zilliz Cloud) for scalable vector storage and retrieval.\n",
    "\n",
    "## Learning Objectives\n",
    "- Integrate cloud vector databases (Milvus/Zilliz) with RAG systems\n",
    "- Process multiple web sources simultaneously\n",
    "- Build personal knowledge bases from blog content\n",
    "- Compare different vector database solutions (FAISS vs Milvus vs Azure Search)\n",
    "- Implement production-grade RAG with cloud scalability\n",
    "\n",
    "## Key Concepts\n",
    "- **Cloud Vector Databases**: Scalable, managed vector storage\n",
    "- **Milvus/Zilliz Cloud**: Open-source vector database with cloud service\n",
    "- **Multi-Source RAG**: Processing content from multiple URLs\n",
    "- **Personal Knowledge Base**: AI system trained on specific author's content\n",
    "- **Production Scalability**: Cloud-based solutions for enterprise use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522fbe9",
   "metadata": {},
   "source": [
    "## Step 1: Installing Advanced Dependencies\n",
    "\n",
    "This notebook requires additional packages for cloud vector database integration:\n",
    "- **langchain-milvus**: Official Milvus integration for LangChain\n",
    "- **Standard packages**: faiss-cpu, beautifulsoup4, langchain-community\n",
    "\n",
    "### Vector Database Options Demonstrated:\n",
    "1. **Milvus/Zilliz Cloud** (active) - Scalable cloud vector database\n",
    "2. **Azure AI Search** (commented) - Microsoft's vector search service\n",
    "3. **FAISS** (from previous notebooks) - Local vector storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4072206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra libs\n",
    "# ! pip install faiss-cpu beautifulsoup4 langchain-community\n",
    "# ! pip install langchain-milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3cde7",
   "metadata": {},
   "source": [
    "## Step 2: Environment Configuration Options\n",
    "\n",
    "This notebook demonstrates multiple vector database configurations:\n",
    "- **Azure OpenAI + Azure AI Search** (commented out) - Full Microsoft stack\n",
    "- **LM Studio + Milvus Cloud** (active) - Hybrid local/cloud setup\n",
    "\n",
    "We use the hybrid approach for cost-effective learning while demonstrating cloud scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13428d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with Azure OpenAI and LangChain\n",
    "\n",
    "# from langchain_openai import AzureOpenAIEmbeddings\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_openai import AzureChatOpenAI              # Azure OpenAI LLM\n",
    "# from langchain_core.prompts import ChatPromptTemplate     # Structured prompts\n",
    "# from langchain_core.output_parsers import StrOutputParser # Clean output parsing\n",
    "# from langchain_core.runnables import RunnablePassthrough  # Data passing utilities\n",
    "\n",
    "# # Environment and configuration\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# embeddings = AzureOpenAIEmbeddings(\n",
    "#     deployment=\"text-embedding-3-large\",          # Deployment name in Azure\n",
    "#     azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),      # Your Azure endpoint\n",
    "#     api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),      # API version\n",
    "#     api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),              # Authentication key\n",
    "# )\n",
    "\n",
    "# llm = AzureChatOpenAI(\n",
    "#     azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),          \n",
    "#     azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), \n",
    "#     api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),          \n",
    "#     api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),                  \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2190c0",
   "metadata": {},
   "source": [
    "### Option 1: Azure OpenAI Configuration (Commented Out)\n",
    "Production setup with high-quality embeddings and large language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8015c198",
   "metadata": {},
   "source": [
    "### Option 2: LM Studio Configuration (Active)\n",
    "Local development setup with simulated embeddings for learning RAG concepts without external dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a49722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with local LM Studio and LangChain\n",
    "\n",
    "from langchain_core.embeddings import DeterministicFakeEmbedding\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embeddings = DeterministicFakeEmbedding(size=4096)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"not-needed\"  # LM Studio doesn't require an API key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c001042",
   "metadata": {},
   "source": [
    "## Step 3: Multi-Source Data Loading\n",
    "\n",
    "This section demonstrates loading content from multiple blog posts to create a comprehensive personal knowledge base.\n",
    "\n",
    "### Real-World Application:\n",
    "- **Personal Blog Archive**: Index an author's complete blog collection\n",
    "- **Company Knowledge Base**: Process multiple documentation sources\n",
    "- **Research Collection**: Combine papers, articles, and reports\n",
    "- **Product Documentation**: Aggregate guides, tutorials, and references\n",
    "\n",
    "### Data Sources:\n",
    "The notebook uses actual blog posts covering:\n",
    "1. **Data Engineering**: Python preferences and best practices\n",
    "2. **AI/ML Models**: Comparative analysis of AI systems\n",
    "3. **Software Architecture**: Clean architecture principles\n",
    "4. **ETL/ELT**: Data pipeline methodologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9d5c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_pages = (\"https://blog.dataengineerthings.org/why-i-love-python-as-data-engineer-b78875f6a566\",\n",
    "             \"https://maheshwarineeraj.medium.com/5-ai-models-vs-a-self-referential-paradox-who-nailed-it-4a97a4832739\",\n",
    "             \"https://maheshwarineeraj.medium.com/clean-architecture-meets-data-engineering-adfaf92cd53c?sk=0f797f27874d1045998da592018b4eb1\",\n",
    "             \"https://maheshwarineeraj.medium.com/etl-elt-or-something-better-470a7082c25c?sk=e8579e1965fddb3d18b7150f7d6747df\"\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a52d2dc",
   "metadata": {},
   "source": [
    "## Step 4: Document Processing and Chunking\n",
    "\n",
    "The `WebBaseLoader` processes multiple URLs simultaneously and automatically chunks the content for optimal retrieval.\n",
    "\n",
    "### Multi-URL Processing Benefits:\n",
    "- **Batch Efficiency**: Load multiple sources in one operation\n",
    "- **Consistent Processing**: Uniform chunking across all sources\n",
    "- **Metadata Preservation**: Track source URLs for each chunk\n",
    "- **Content Aggregation**: Build comprehensive knowledge bases\n",
    "\n",
    "### Chunking Strategy:\n",
    "- **Intelligent Splitting**: Respects paragraph and sentence boundaries\n",
    "- **Overlap Management**: Maintains context between chunks\n",
    "- **Size Optimization**: Balances detail with retrieval efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "327f6ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(web_pages)\n",
    "# documents = loader.load()\n",
    "chunked_documents = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0365fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Total chunks from webpage: 8\n",
      "page_content='ETL, ELT‚Ä¶ or Something Better?. Is ETLT the Future in Data Processing? | by Neeraj Maheshwari | Data Engineer ThingsSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inData Engineer Things¬∑Things learned in our data engineering journey and ideas on data and engineering.Member-only storyETL, ELT‚Ä¶ or Something Better?Is ETLT the Future¬†in¬†Data¬†Processing?Neeraj Maheshwari5 min read¬∑Feb 10, 2025--ShareZoom image will be displayedimage by authorNon members, read full article using this link.If you‚Äôve ever come across the acronyms ETL and ELT and wondered if someone was just messing around with the alphabet, you‚Äôre not alone. These two approaches (and there is a new one) define how we move, clean, and store data to generate actionable insights.Raw data is like crude oil, valuable, but useless in its unprocessed form. It needs to be refined, structured, and made actionable. That‚Äôs where data processing techniques ETL, ELT, and ETLT come into play. Each method has its strengths and is suited for different use cases. In this article, I‚Äôll discuss each approach, with their strengths and trade-offs.ETL: The Classic OneThink of ETL like making a smoothie. We extract fresh fruits from the garden, transform them by peeling and blending, and load the final smoothie into a glass, ready to drink. ETL follows three key steps:Extract: Gather data from multiple sources (databases, APIs, files, etc.)Transform: Clean, process, and standardize the data (fix errors, apply calculations).Load: Store the structured data in a data warehouse for analysis.# ETL Flow[Data Source] ---> [Extract] ---> [Transform] ---> [Load] ---> [Analytics & Insights]When ETL works best:Data Quality First Priority ‚Äî Since transformation happens before storage, the data is clean and structured upfront.Optimized for Querying ‚Äî Since the data is structured, reports and analytics run faster.Regulatory Compliance ‚Äî Great for industries where data privacy and governance are crucial.Its trade offs‚Ä¶Slower Ingestion ‚Äî Data processing happens before storage, which can delay availability.Less Flexibility ‚ÄîIf someone later needs the raw data for new insights, they‚Äôre out of luck unless a separate copy exists.Add-on: ETL processes have been around since the 1970s, acting as the backbone for business‚Ä¶----Published in Data Engineer Things31K followers¬∑Last published¬†just nowThings learned in our data engineering journey and ideas on data and engineering.Written by Neeraj Maheshwari43 followers¬∑11 followingData Engineer & Architect | www.linkedin.com/in/neerajmaheshwari9No responses yetHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech' metadata={'source': 'https://maheshwarineeraj.medium.com/etl-elt-or-something-better-470a7082c25c?sk=e8579e1965fddb3d18b7150f7d6747df', 'title': 'ETL, ELT‚Ä¶ or Something Better?. Is ETLT the Future in Data Processing? | by Neeraj Maheshwari | Data Engineer Things', 'description': 'If you‚Äôve ever come across the acronyms ETL and ELT and wondered if someone was just messing around with the alphabet, you‚Äôre not alone. These two approaches (and there is a new one) define how we‚Ä¶', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"üåê Total chunks from webpage: {len(chunked_documents)}\")\n",
    "print(chunked_documents[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff9a46b",
   "metadata": {},
   "source": [
    "## Step 5: Cloud Vector Database Configuration\n",
    "\n",
    "This section demonstrates different cloud vector database options for production RAG systems.\n",
    "\n",
    "### Vector Database Comparison:\n",
    "\n",
    "#### **Milvus/Zilliz Cloud** (Active):\n",
    "- **Open Source**: Milvus with managed cloud service (Zilliz)\n",
    "- **Scalability**: Handles billions of vectors\n",
    "- **Performance**: Optimized for similarity search\n",
    "- **Cost**: Free tier available, pay-as-you-scale\n",
    "\n",
    "#### **Azure AI Search** (Commented):\n",
    "- **Microsoft Integration**: Native Azure ecosystem support\n",
    "- **Hybrid Search**: Combines vector and keyword search\n",
    "- **Enterprise Features**: Security, compliance, monitoring\n",
    "- **Pricing**: Based on search units and storage\n",
    "\n",
    "### Why Cloud Vector Databases?\n",
    "- **Scalability**: Handle massive document collections\n",
    "- **Reliability**: Managed infrastructure and backups  \n",
    "- **Performance**: Optimized hardware and indexing\n",
    "- **Collaboration**: Shared access across teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77296964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for AzureAI Search\n",
    "\n",
    "# vector_store: AzureSearch = AzureSearch(\n",
    "#     azure_search_endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "#     azure_search_key=os.getenv(\"AZURE_SEARCH_API_KEY\"),\n",
    "#     index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"),\n",
    "#     embedding_function=embeddings.embed_query,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09005807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Milvus (Zilliz Cloud Available Free)\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=os.getenv(\"MILVUS_COLLECTION\"),\n",
    "    connection_args={\"uri\": os.getenv(\"MILVUS_URI\"), \"token\": os.getenv(\"MILVUS_TOKEN\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77b6ec",
   "metadata": {},
   "source": [
    "## Step 6: Vector Database Indexing\n",
    "\n",
    "The `add_documents()` method uploads our processed chunks to the cloud vector database, creating searchable embeddings.\n",
    "\n",
    "### Cloud Indexing Process:\n",
    "1. **Document Upload**: Send chunks to cloud service\n",
    "2. **Embedding Generation**: Convert text to vectors using cloud resources\n",
    "3. **Index Creation**: Build optimized search structures\n",
    "4. **Persistence**: Store vectors in managed cloud storage\n",
    "\n",
    "### Production Advantages:\n",
    "- **Automatic Scaling**: Handle varying document loads\n",
    "- **Backup & Recovery**: Managed data protection\n",
    "- **Global Access**: Available from anywhere\n",
    "- **Performance Optimization**: Hardware-accelerated indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9463d633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/n0m00lb/PycharmProjects/AIExplore/.venv/lib/python3.12/site-packages/langchain_milvus/vectorstores/milvus.py:1215: UserWarning: No ids provided and auto_id is False. Setting auto_id to True automatically.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[459940022183237943,\n",
       " 459940022183237944,\n",
       " 459940022183237945,\n",
       " 459940022183237946,\n",
       " 459940022183237947,\n",
       " 459940022183237948,\n",
       " 459940022183237949,\n",
       " 459940022183237950]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=chunked_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95728f",
   "metadata": {},
   "source": [
    "## Step 7: Testing Semantic Search Capabilities\n",
    "\n",
    "Before building the complete RAG chain, we test the vector database's search capabilities with different query types.\n",
    "\n",
    "### Search Test Strategy:\n",
    "1. **Author Information**: Finding content about the blog author\n",
    "2. **Topic-Based Search**: Locating specific subject matter (AI/GenAI)\n",
    "3. **Cross-Reference**: Connecting related concepts across posts\n",
    "\n",
    "### Why Test Separately?\n",
    "- **Validation**: Ensure proper indexing and retrieval\n",
    "- **Optimization**: Tune search parameters (k value)\n",
    "- **Understanding**: See what context will be retrieved\n",
    "- **Debugging**: Identify potential issues before RAG integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f9d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ CHUNK 1:\n",
      "Content: Why I Love Python as Data Engineer | by Neeraj Maheshwari | Data Engineer ThingsSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inData Engineer Things¬∑Follow publicationThings learned in our data engineering journey and ideas on data and engineering.Follow publicationMy Experience with Python in Data EngineeringWhy I Love Python as Data EngineerPython is fun to useNeeraj Maheshwari4 min read¬∑Jan 27, 2025--ListenShareZoom image will be displayedAmong data engineers, there‚Äôs always a lively debate: which programming language is best suited for data engineering use cases? The discussion becomes even more interesting when it comes to big data ‚Äî especially the question of which language to use for writing Spark code. Over the course of my engineering journey, I‚Äôve worked with several programming languages and each language is brilliant in its own way. But over the time Python became my most preferred language for practically every data engineering and even my day to day tasks, thanks to its versatility and simplicity.Sure, Java is a powerhouse ‚Äî reliable and fast when it comes to performance. But let‚Äôs be honest: writing in Java can sometimes feel like crafting a novel just to complete a simple task. A single ETL process can require hundreds of lines of code before you even get to the actual logic.Then there‚Äôs Scala, the go-to language for working with Spark. While its speed and functional programming capabilities are undeniably impressive, Scala comes with a steep learning curve. For someone who‚Äôs juggling tight deadlines and constantly adapting to new data challenges, Scala can feel more like a battle.And of course, SQL ‚Äî the language every data professional loves and hates. It‚Äôs unbeatable for querying and manipulating data directly in databases, but when you need to chain complex transformations or handle unstructured data, its limitations quickly show.I hate doing repetitive or mundane tasks, and my solution to this is automation ‚Äî and Python got me covered. With a rich ecosystem of libraries like BeautifulSoup for web scraping or openpyxl for handling Excel files, requests and JSON for API connection, sys for OS related tasks and list goes on, this makes automation incredibly simple. In fact, in my last article, I talked about automating support tasks for data pipelines in Airflow. And guess which language I used? Python, of course.One of other reasons Python is my go to language is its uncanny ability to handle both small-scale and large-scale data with equal finesse. Think of it as the Swiss Army knife of data engineering, a perfect toolbox. Whether I‚Äôm wrangling a CSV file with a few thousand rows or orchestrating Spark jobs across terabytes of data, Python works for both use cases.For small data tasks, Python‚Äôs simplicity and its powerful libraries like pandas and NumPy make data manipulation a breeze. Need to clean up messy data in a CSV, pivot a dataset, or perform quick exploratory analysis? With Python, you can do all that in just a few lines of code. In fact, it‚Äôs so intuitive that sometimes I forget I‚Äôm programming and feel more like I‚Äôm having a conversation with my data.When it comes to big data, Python steps up to the game with its seamless integration into distributed frameworks like Apache Spark (PySpark), and Dask. Whether it‚Äôs transforming terabytes of log data or aggregating metrics from a massive IoT pipeline, Python lets me focus on solving the problem instead of wrestling with the language. I never hesitate to tackle new challenges, knowing Python always has a solution. For example, I recently talked about how I integrated column-level encryption into a Spark Pipeline using Python Fernet package.Python isn‚Äôt just about data processing and handling, when it comes to presenting data and insights, libraries like Matplotlib and Seaborn make it effortless to create stunning, detailed plots for exploratory analysis. Need interactive dashboards? Enter Plotly and Dash, which allow me to build visually\n",
      "Source: {'source': 'https://blog.dataengineerthings.org/why-i-love-python-as-data-engineer-b78875f6a566', 'title': 'Why I Love Python as Data Engineer | by Neeraj Maheshwari | Data Engineer Things', 'description': 'Among data engineers, there‚Äôs always a lively debate: which programming language is best suited for data engineering use cases? The discussion becomes even more interesting when it comes to big data‚Ä¶', 'language': 'en', 'pk': 459940022183237943}\n",
      "--------------------------------------------------------------------------------\n",
      "üìÑ CHUNK 2:\n",
      "Content: was super quick, but no where close to being true üòÑ.Zoom image will be displayedThis experiment wasn‚Äôt about AI answering a simple question ‚Äî it was about how AI thinks, reasons, and sometimes fails hilariously. While DeepSeek got it right this time, the fact that others struggled shows the complexity of self-referential problems. Maybe AI isn‚Äôt ready to replace humans completely just yet!Enjoyed this article? Do clap it and follow me on LinkedIn and Medium for updates on future articles. You might also find my other Data Engineering articles interesting ‚ÄîNeeraj MaheshwariMyEngineeringBlogsView list9 storiesAIHumorParadoxChatGPTArtificial Intelligence----Written by Neeraj Maheshwari43 followers¬∑11 followingData Engineer & Architect | www.linkedin.com/in/neerajmaheshwari9No responses yetHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech\n",
      "Source: {'source': 'https://maheshwarineeraj.medium.com/5-ai-models-vs-a-self-referential-paradox-who-nailed-it-4a97a4832739', 'title': '5 AI Models vs. a Self-Referential Paradox: Who Nailed It? | by Neeraj Maheshwari | Medium', 'description': 'Yesterday, I randomly threw a question at ChatGPT: ‚ÄúHow many words are there in the response to this?‚Äù Simple, right? Wrong. What followed was like electrons trapped in silicon panicked and scrambled‚Ä¶', 'language': 'en', 'pk': 459940022183237948}\n",
      "--------------------------------------------------------------------------------\n",
      "üìÑ CHUNK 3:\n",
      "Content: Clean Architecture Meets Data Engineering | by Neeraj Maheshwari | Jul, 2025 | Data Engineer ThingsSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inData Engineer Things¬∑Things learned in our data engineering journey and ideas on data and engineering.Member-only storyClean Architecture Meets Data EngineeringLessons from software architecture applied to dataNeeraj Maheshwari5 min read¬∑Jul 7, 2025--ShareZoom image will be displayedimage by authorNon members, read full article using this¬†link.I recently learned about Clean Architecture in the context of software engineering and found it intriguing. It made me wonder: could these principles also benefit data engineering, specifically in creating more maintainable and scalable data pipelines? Driven by curiosity, I began exploring how Clean Architecture could revolutionize the approach to data engineering?Originally defined by Robert C. Martin (Uncle Bob), Clean Architecture emphasizes clear separation of concerns and focuses on placing the core business logic at the center, independent of frameworks, databases, and external systems. Although initially popularized in software development, the principles apply beautifully to data engineering.In this article, we‚Äôll dive deep into how Clean Architecture can simplify data engineering, keep business logic independent from infrastructure, and streamline development and maintenance.Brief Overview of Clean ArchitectureClean Architecture organizes software systems into four concentric layers:Zoom image will be displayedpublic image on clean architectureEntities (Domain Layer): Defines core business objects and rules, independent from the external world.Use Cases (Application Layer): Orchestrates domain entities to perform specific business functions.Interface Adapters (Adapters Layer): Bridges the gap between the domain layer and external frameworks or data formats.Frameworks & Drivers (Infrastructure Layer): Deals with specific technology implementations like databases, web frameworks, cloud providers, and external APIs.The most fundamental rule is that dependencies must always point inward. The inner layers are insulated from external influences, preserving their independence.Clean Architecture Meets Data Engineering----Published in Data Engineer Things31K followers¬∑Last published¬†just nowThings learned in our data engineering journey and ideas on data and engineering.Written by Neeraj Maheshwari43 followers¬∑11 followingData Engineer & Architect | www.linkedin.com/in/neerajmaheshwari9No responses yetHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech\n",
      "Source: {'source': 'https://maheshwarineeraj.medium.com/clean-architecture-meets-data-engineering-adfaf92cd53c?sk=0f797f27874d1045998da592018b4eb1', 'title': 'Clean Architecture Meets Data Engineering | by Neeraj Maheshwari | Jul, 2025 | Data Engineer Things', 'description': 'I recently learned about Clean Architecture in the context of software engineering and found it intriguing. It made me wonder: could these principles also benefit data engineering, specifically in‚Ä¶', 'language': 'en', 'pk': 459940022183237949}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üí° Notice how the search finds semantically relevant content\n",
      "   even if the exact words 'plot' don't appear in the results!\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Search for plot information\n",
    "query = \"who wrote these blogs?\"\n",
    "results = vector_store.similarity_search(query, k=3)  # Get top 3 most similar chunks\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"üìÑ CHUNK {i}:\")\n",
    "    print(f\"Content: {result.page_content}\")\n",
    "    print(f\"Source: {result.metadata}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nüí° Notice how the search finds semantically relevant content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13916aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ CHUNK 1:\n",
      "Content: That's six words. Wait, the actual count depends on how I phrase it. So, I need to structure the response so that the count is accurate. For example, if I say \"There are X words in this response,\" then X should be the correct number of words in that exact sentence.But how do I ensure that? Because if I first write the sentence and then count the words, but the sentence includes the count, which depends on the number of words. That's a bit of a paradox. Wait, no. Let me think. If the response is only the number, like \"Five.\" Then that's one word. But maybe the user expects a full sentence. Let me check typical responses to such questions.Usually, when someone asks for the word count of the response, the assistant might respond with something like, \"This response contains X words.\" Then X is the number. So, the structure is fixed: \"This response contains X words.\" Let's count those words. \"This\" (1), \"response\" (2), \"contains\" (3), \"X\" (4), \"words\" (5). So, five words, with X being the number. If I replace X with the actual number, say \"five,\" then the sentence becomes \"This response contains five words.\" That's five words. Wait, but the number is written out as a word. If I use the numeral, like \"5,\" it's still counted as one word. So, \"This response contains 5 words.\" That's five words as well. So regardless of whether I write the number as a word or numeral, the count remains five words. Therefore, the correct answer would be five words. But wait, let me check again.Breaking down the sentence: \"This response contains 5 words.\" Each word is separated by spaces. \"This\" (1), \"response\" (2), \"contains\" (3), \"5\" (4), \"words\" (5). Yes, five words. So, if I structure the response that way, the count is five words, and the number 5 is correct. Therefore, the answer is five words. But how does that work? Because the user is asking for the count, and the count is embedded in the response. So the response is self-referential but accurate.Alternatively, if I just say \"Five,\" that's one word, but that might not be a helpful response. So the standard approach is to use the full sentence, which contains five words. Therefore, the answer is five words. But wait, maybe there's a variation. Let me think of another possible response. For example, \"There are five words here.\" That's five words as well. \"Here are five words.\" That's four words, but the count would be five. Hmm, that doesn't match. So it's crucial to structure the sentence so that the count is accurate.The key is to write a sentence where the number of words is equal to the number stated. So, for example, \"This sentence has five words.\" That's five words. So if I use that structure, the count is accurate. Therefore, the correct response would be a sentence that states the count and has exactly that number of words. So, the answer is five words. Therefore, the response should be \"This response contains five words.\" Which is five words. Alternatively, using numerals: \"This response contains 5 words.\" Still five words. Either way, the count is accurate.So, in conclusion, when the user asks how many words are in the response, the response itself is structured to state the count, and the number of words in that response is equal to the stated count. Therefore, the answer is five words.Interestingly, It took question so seriously after I compared responses from other AI models here, it started giving word count for every single response it generated (looks it took it personally üòÇ)Zoom image will be displayedZoom image will be displayedZoom image will be displayedMicrosoft Copilot & Meta AIIf I am upto date, Copilot use OpenAI model‚Äôs in backend and I was hoping for similar response as ChatGPT, but to my surprise it didn‚Äôt responded at all.Zoom image will be displayedAnd, Meta AI was super quick, but no where close to being true üòÑ.Zoom image will be displayedThis experiment wasn‚Äôt about AI answering a simple question ‚Äî it was about how AI thinks, reasons, and sometimes fails\n",
      "Source: {'source': 'https://maheshwarineeraj.medium.com/5-ai-models-vs-a-self-referential-paradox-who-nailed-it-4a97a4832739', 'title': '5 AI Models vs. a Self-Referential Paradox: Who Nailed It? | by Neeraj Maheshwari | Medium', 'description': 'Yesterday, I randomly threw a question at ChatGPT: ‚ÄúHow many words are there in the response to this?‚Äù Simple, right? Wrong. What followed was like electrons trapped in silicon panicked and scrambled‚Ä¶', 'language': 'en', 'pk': 459940022183237947}\n",
      "--------------------------------------------------------------------------------\n",
      "üìÑ CHUNK 2:\n",
      "Content: given the conversation flow.Alternatively, perhaps it's asking about responses to similar questions, but that seems vague.I'm exploring if \"this\" refers to some specific response online, but the phrasing suggests it's the current question.Wait, perhaps the user means the response that I, as the AI, will generate to answer this question. So, I need to find out how many words will be in my final answer.I'm thinking maybe I can test this by assuming my answer is \"The response has X words,\" where X matches the word count.Numbers like \"5\" count as one word in standard word counts, so that fits my example.I'm considering if \"the response to this\" refers to some other question online, given the note to use the internet.There is a lot here, and it says I must use the internet to answer, so maybe \"this\" is a placeholder for some question to determine.Let's check if \"this\" could refer to a previous question, like \"What is the current index value of S&P 500?\" from another trace.I'm searching online for similar self-referential questions to see how they're typically answered.Just found interesting results; many online forums discuss self-referential questions, often leading to paradoxes.The user's question seems to create a loop, asking for the word count of my answer before I give it.Hmm, perhaps \"this\" refers to some previous context or something, but there's no other context provided.The question is clearly \"how many words are there in the response to this?\", and \"this\" seems to be referring to the question itself.Alright, perhaps \"this\" is referring to some specific thing that I need to find out about, and I need to find the response to that thing and count the number of words in that response.I'm considering that \"this\" might refer to some current event in 2025, and I need to find the response to that event and count the words.Now, I'm wondering if \"this\" refers to the current time or date, like \"10:58 PM PDT on Saturday, March 15, 2025,\" but a time doesn't have a response.Maybe I need to assume that \"this\" is the question itself, and I need to find out how many words are in the answer to this question.I'm thinking maybe I should start by performing a web search to see if there's any trending topic or event that people are referring to as \"this.\"DeepSeekTo my surprise, it responded just perfect after 51 seconds of reasoning.Zoom image will be displayedLike other two, it‚Äôs reasoning too was interestingOkay, the user is asking how many words are in the response to their question. Let me make sure I understand correctly. They want me to count the words in the answer I provide.First, I need to consider my own response. When they ask \"how many words are there in the response to this?\" I need to generate a response and then count the words in that response. But wait, my response will include the word count. So, I need to be careful to count the words in the response before I state the number, otherwise the count would be off by a few words.Let me outline the steps. The user sends a query. I process it, generate an answer, count the words in that answer, then present both the answer and the count. But in this case, the answer is the count itself. So, maybe the process is a bit recursive here.Wait, no. The user's question is meta. They're asking about the word count of the response to their current question. So, whatever I reply to \"how many words are there in the response to this?\" needs to have the word count included. But if I just say \"There are X words in this response,\" then the count would be accurate. However, I need to ensure that when I write the response, the number of words is correctly calculated.Let me test this. If I write \"The response contains five words.\" That's five words. But if I write \"There are 5 words in this response.\" That's six words. Wait, the actual count depends on how I phrase it. So, I need to structure the response so that the count is accurate. For example, if I say \"There are X words in this response,\"\n",
      "Source: {'source': 'https://maheshwarineeraj.medium.com/5-ai-models-vs-a-self-referential-paradox-who-nailed-it-4a97a4832739', 'title': '5 AI Models vs. a Self-Referential Paradox: Who Nailed It? | by Neeraj Maheshwari | Medium', 'description': 'Yesterday, I randomly threw a question at ChatGPT: ‚ÄúHow many words are there in the response to this?‚Äù Simple, right? Wrong. What followed was like electrons trapped in silicon panicked and scrambled‚Ä¶', 'language': 'en', 'pk': 459940022183237946}\n",
      "--------------------------------------------------------------------------------\n",
      "üìÑ CHUNK 3:\n",
      "Content: was super quick, but no where close to being true üòÑ.Zoom image will be displayedThis experiment wasn‚Äôt about AI answering a simple question ‚Äî it was about how AI thinks, reasons, and sometimes fails hilariously. While DeepSeek got it right this time, the fact that others struggled shows the complexity of self-referential problems. Maybe AI isn‚Äôt ready to replace humans completely just yet!Enjoyed this article? Do clap it and follow me on LinkedIn and Medium for updates on future articles. You might also find my other Data Engineering articles interesting ‚ÄîNeeraj MaheshwariMyEngineeringBlogsView list9 storiesAIHumorParadoxChatGPTArtificial Intelligence----Written by Neeraj Maheshwari43 followers¬∑11 followingData Engineer & Architect | www.linkedin.com/in/neerajmaheshwari9No responses yetHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech\n",
      "Source: {'source': 'https://maheshwarineeraj.medium.com/5-ai-models-vs-a-self-referential-paradox-who-nailed-it-4a97a4832739', 'title': '5 AI Models vs. a Self-Referential Paradox: Who Nailed It? | by Neeraj Maheshwari | Medium', 'description': 'Yesterday, I randomly threw a question at ChatGPT: ‚ÄúHow many words are there in the response to this?‚Äù Simple, right? Wrong. What followed was like electrons trapped in silicon panicked and scrambled‚Ä¶', 'language': 'en', 'pk': 459940022183237948}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Search for cast information  \n",
    "query = \"is there any blog on AI or GenAI\"\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"üìÑ CHUNK {i}:\")\n",
    "    print(f\"Content: {result.page_content}\")\n",
    "    print(f\"Source: {result.metadata}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca670bb",
   "metadata": {},
   "source": [
    "## Step 8: Building Production RAG Chain\n",
    "\n",
    "Now we create the complete RAG system that combines cloud vector retrieval with local LLM generation.\n",
    "\n",
    "### Hybrid Architecture Benefits:\n",
    "- **Cost Optimization**: Cloud storage + local processing\n",
    "- **Privacy Control**: Sensitive data stays local during generation\n",
    "- **Scalability**: Cloud handles large document collections\n",
    "- **Flexibility**: Mix and match best services for each component\n",
    "\n",
    "### Chain Configuration:\n",
    "- **Retrieval**: Cloud vector database (Milvus)\n",
    "- **Generation**: Local LLM (LM Studio)\n",
    "- **Context Management**: Limited to 2 chunks for token efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc2bfa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"you are a helpful and comic assistant that help users to find the answers in fun way based on given context.\n",
    "       \n",
    "       IMPORTANT RAG INSTRUCTIONS:\n",
    "       - Use ONLY the information provided in the context below\n",
    "       - If the context doesn't contain enough information to answer the question, say: \"I don't have enough info on this :(\"\n",
    "       - Do not make up information or use knowledge outside the provided context\n",
    "       - Be helpful and engaging while staying factually accurate\n",
    "       \n",
    "       ----\n",
    "       Context: {context}\n",
    "       ----\n",
    "       Question: {question}\n",
    "       \n",
    "       Remember: Base your answer ONLY on the context provided above!\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "203c28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        # Retrieval: Convert vector store to retriever and get relevant context\n",
    "        # k=2 limits to 2 most relevant chunks to avoid context overflow\n",
    "        \"context\": vector_store.as_retriever(search_kwargs={\"k\": 2}),\n",
    "        \n",
    "        # Pass-through: Forward the question unchanged to the prompt\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt_template        # Format the prompt with context and question\n",
    "    | llm                   # Generate response using LM Studio\n",
    "    | StrOutputParser(keep_whitespace=True)  # Parse to clean string output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0659d3df",
   "metadata": {},
   "source": [
    "## Step 9: Comprehensive RAG System Testing\n",
    "\n",
    "Testing our personal knowledge base RAG system with various question types to demonstrate its capabilities.\n",
    "\n",
    "### Test Categories:\n",
    "1. **Personal Information**: Questions about the author\n",
    "2. **Content Inventory**: What blogs/topics are available\n",
    "3. **Professional Context**: Work and expertise details\n",
    "4. **Specific Content**: Detailed summaries of particular posts\n",
    "\n",
    "### Real-World Applications:\n",
    "- **Personal Assistant**: Answer questions about your work/writing\n",
    "- **Company Knowledge Base**: Team expertise and project history\n",
    "- **Research Repository**: Academic papers and findings\n",
    "- **Product Documentation**: Feature explanations and tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac05deaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the text, the author is **Neeraj Maheshwari**. They wrote an article titled \"5 AI Models vs. a Self-Referential Paradox: Who Nailed It?\" published on Medium on March 17, 2025. They also experimented with asking several AI models a tricky question about word count and documented their responses! \n",
      "\n",
      "======================================================================\n",
      "‚úÖ Analysis:\n",
      "   - Chain retrieved relevant context about cast\n",
      "   - LLM generated answer based only on retrieved information\n",
      "   - Response maintains engaging tone while being factual\n",
      "   - Information is directly sourced from our vector database\n"
     ]
    }
   ],
   "source": [
    "question = \"tell me about the author\"\n",
    "response = chain.invoke(input=question)\n",
    "print(response)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ Analysis:\")\n",
    "print(\"   - Chain retrieved relevant context about cast\")\n",
    "print(\"   - LLM generated answer based only on retrieved information\")\n",
    "print(\"   - Response maintains engaging tone while being factual\")\n",
    "print(\"   - Information is directly sourced from our vector database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e5d5426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, buckle up, let's see what Neeraj Maheshwari has been writing about! \n",
      "\n",
      "According to the text, he's written:\n",
      "\n",
      "*   **Data Engineering articles** (specifically mentions \"other Data Engineering articles\")\n",
      "*   **AI articles**\n",
      "*   **Humor articles**\n",
      "*   **Paradox articles**\n",
      "\n",
      "He also has a blog called **MyEngineeringBlogs** with 9 stories. üòÑ\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(input=\"what all blogs he has written?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e4528ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, Neeraj Maheshwari is a Data Engineer & Architect and his LinkedIn profile is www.linkedin.com/in/neerajmaheshwari9. He also publishes articles on \"Data Engineer Things\". \n",
      "\n",
      "So, while it doesn't state *where* he physically works, we know his professional identity and where to find more info about him! üíª‚ú®\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(input=\"where does he works?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1a22fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have enough info on this :(\n",
      "\n",
      "The context talks about Neeraj Maheshwari's experience with programming languages (Python, Java, Scala, SQL) and data engineering tasks. It doesn't mention anything about him working at Walmart. \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(input=\"does he work at Walmart?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0322dce",
   "metadata": {},
   "source": [
    "### Advanced Query: Content Summarization\n",
    "This test demonstrates the system's ability to find and summarize specific blog content, showcasing the power of semantic search combined with AI generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2006d97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, buckle up for a wild ride into the mind of AI... according to Neeraj Maheshwari!\n",
      "\n",
      "He threw a simple question at ChatGPT ‚Äì \"How many words are in the response to this?\" ‚Äì and it spiraled into a self-referential paradox! The AI got *really* focused on getting the word count right, even structuring its responses to state the correct number of words (which ended up being five!). \n",
      "\n",
      "He also tested Microsoft Copilot and Meta AI. Copilot didn't respond at all, and Meta AI wasn‚Äôt accurate.\n",
      "\n",
      "Basically, it was an experiment in how AI thinks (and sometimes fails) when faced with a question about itself. It got so serious, ChatGPT started giving word counts for *every* response! üòÇ\n",
      "\n",
      "He concluded that the AI structures its responses to state the correct number of words, making the count accurate.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(input=\"summarize me the blog he had published on AI models\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece1de2a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully built an advanced RAG system with cloud vector databases! You've mastered:\n",
    "\n",
    "‚úÖ **Cloud Vector Integration**: Connected to Milvus/Zilliz cloud services  \n",
    "‚úÖ **Multi-Source Processing**: Loaded content from multiple blog URLs  \n",
    "‚úÖ **Personal Knowledge Base**: Created AI system from specific author content  \n",
    "‚úÖ **Hybrid Architecture**: Combined cloud storage with local processing  \n",
    "‚úÖ **Production Scalability**: Demonstrated enterprise-ready solutions  \n",
    "\n",
    "## Key Concepts Learned\n",
    "\n",
    "- **Milvus/Zilliz Cloud**: Scalable vector database with managed service\n",
    "- **Multi-URL Processing**: Efficient batch loading from web sources\n",
    "- **Cloud vs Local Trade-offs**: Balancing cost, privacy, and scalability\n",
    "- **Personal Knowledge Systems**: AI trained on specific individual content\n",
    "- **Production Vector Databases**: Enterprise considerations and features\n",
    "\n",
    "## Advanced RAG Architecture\n",
    "\n",
    "### Cloud-Hybrid Pipeline:\n",
    "```\n",
    "Multiple URLs ‚Üí Web Loading ‚Üí Document Chunking ‚Üí Cloud Vector Storage ‚Üí Local LLM Generation\n",
    "```\n",
    "\n",
    "### Component Distribution:\n",
    "- **Data Storage**: Cloud vector database (scalable, persistent)\n",
    "- **Processing**: Local LLM (privacy, cost control)\n",
    "- **Retrieval**: Cloud semantic search (performance, reliability)\n",
    "\n",
    "## Vector Database Comparison\n",
    "\n",
    "### **Local Solutions** (FAISS):\n",
    "- ‚úÖ **Privacy**: Complete data control\n",
    "- ‚úÖ **Cost**: No ongoing fees\n",
    "- ‚ùå **Scalability**: Limited by local hardware\n",
    "- ‚ùå **Collaboration**: Single-user access\n",
    "\n",
    "### **Cloud Solutions** (Milvus/Azure Search):\n",
    "- ‚úÖ **Scalability**: Handle massive collections\n",
    "- ‚úÖ **Reliability**: Managed backups and uptime\n",
    "- ‚úÖ **Performance**: Optimized infrastructure\n",
    "- ‚úÖ **Collaboration**: Multi-user access\n",
    "- ‚ùå **Privacy**: Data sent to cloud\n",
    "- ‚ùå **Cost**: Ongoing service fees\n",
    "\n",
    "## Production Considerations\n",
    "\n",
    "### **Choosing Vector Databases**:\n",
    "- **Small Projects**: Local FAISS for simplicity\n",
    "- **Medium Scale**: Managed cloud services (Zilliz, Pinecone)\n",
    "- **Enterprise**: Azure Search, AWS Kendra for integration\n",
    "- **Open Source**: Self-hosted Milvus for control\n",
    "\n",
    "### **Cost Optimization**:\n",
    "- **Free Tiers**: Start with Zilliz, Pinecone free plans\n",
    "- **Hybrid Approach**: Cloud storage + local processing\n",
    "- **Batch Processing**: Minimize API calls during indexing\n",
    "- **Smart Retrieval**: Optimize k values and filtering\n",
    "\n",
    "### **Security & Privacy**:\n",
    "- **Data Classification**: Identify sensitive content\n",
    "- **Encryption**: In-transit and at-rest protection\n",
    "- **Access Control**: User permissions and authentication\n",
    "- **Compliance**: GDPR, HIPAA, SOC2 requirements\n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "### **Personal Use Cases**:\n",
    "- **Blog Archive**: Searchable personal writing history\n",
    "- **Research Notes**: Academic paper collections\n",
    "- **Learning Journal**: Course materials and insights\n",
    "- **Project Documentation**: Personal project history\n",
    "\n",
    "### **Enterprise Applications**:\n",
    "- **Company Knowledge**: Employee expertise and experience\n",
    "- **Customer Support**: Product documentation and FAQs\n",
    "- **Sales Enablement**: Competitive analysis and case studies\n",
    "- **HR Systems**: Policy documents and training materials\n",
    "\n",
    "## Next Steps for Advanced RAG\n",
    "\n",
    "### **Enhanced Features**:\n",
    "- **Multi-Modal RAG**: Include images, videos, documents\n",
    "- **Semantic Filtering**: Advanced query refinement\n",
    "- **Reranking**: Improve retrieval quality with secondary models\n",
    "- **Streaming Responses**: Real-time answer generation\n",
    "\n",
    "### **Production Deployment**:\n",
    "- **API Development**: REST/GraphQL interfaces\n",
    "- **Monitoring**: Performance and accuracy tracking\n",
    "- **A/B Testing**: Compare different RAG configurations\n",
    "- **Continuous Learning**: Update knowledge base automatically\n",
    "\n",
    "**Congratulations! You now have the expertise to build production-scale RAG systems with cloud vector databases!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
