{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d5c1a62",
   "metadata": {},
   "source": [
    "# LangChain Basics with LM Studio\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the fundamentals of LangChain integration with local LM Studio. You'll learn:\n",
    "- Setting up a Python environment for LangChain\n",
    "- Installing required dependencies in venv\n",
    "- Connecting to LM Studio for local AI inference\n",
    "- Making basic LLM calls and understanding responses\n",
    "\n",
    "## Prerequisites\n",
    "- LM Studio installed and running\n",
    "- Python 3.8+ environment\n",
    "- Basic understanding of Python \n",
    "\n",
    "## Learning Objectives\n",
    "- Understand LangChain's role in AI applications\n",
    "- Configure local AI models using LM Studio\n",
    "- Make your first AI model calls\n",
    "- Interpret AI model responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55a05b",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "Creating a virtual environment isolates your project dependencies and prevents conflicts with other Python projects. This is a best practice for Python development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12347271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create python virtual env and install dependencies\n",
    "\n",
    "#! python -m venv .venv\n",
    "#! source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982886fc",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies\n",
    "\n",
    "LangChain requires several packages:\n",
    "- **langchain**: Core framework for building AI applications\n",
    "- **langchain-openai**: OpenAI-compatible interface (works with LM Studio)\n",
    "- **langchain-community**: Additional community tools and integrations\n",
    "- **python-dotenv**: Environment variable management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba0991ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "# ! pip install langchain langchain-openai langchain-community python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5a574e",
   "metadata": {},
   "source": [
    "## Step 3: Import Required Libraries\n",
    "\n",
    "Import the necessary components for building LangChain applications. We import both Azure OpenAI (for cloud usage) and ChatOpenAI (for local LM Studio usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325daa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc5468d",
   "metadata": {},
   "source": [
    "## Step 4: Load Environment Variables\n",
    "\n",
    "Environment variables keep sensitive information like API keys secure and separate from your code. The `.env` file should never be committed to version control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24ab5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c637b297",
   "metadata": {},
   "source": [
    "## Step 5: Azure OpenAI Configuration (If Credentials Available)\n",
    "\n",
    "This section shows how to configure Azure OpenAI for cloud-based AI inference. The configuration is commented out since we'll be using LM Studio for local inference instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e551932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure OpenAI chat model with environment variables\n",
    "# Ensure you have set the following environment variables:\n",
    "# AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT_NAME, AZURE_OPENAI_API_VERSION, AZURE_OPENAI_API_KEY\n",
    "# These can be set in a .env file or directly in your environment.\n",
    "# Example .env file content:\n",
    "# AZURE_OPENAI_ENDPOINT=https://your-azure-openai-endpoint.openai.azure.com\n",
    "# AZURE_OPENAI_DEPLOYMENT_NAME=your-deployment-name\n",
    "# AZURE_OPENAI_API_VERSION=2023-05-15\n",
    "# AZURE_OPENAI_API_KEY=your-api-key \n",
    "\n",
    "\n",
    "\n",
    "# llm = AzureChatOpenAI(\n",
    "#     azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),          # Azure OpenAI endpoint URL\n",
    "#     azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), # Deployment name (model)\n",
    "#     api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),          # API version\n",
    "#     api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),                  # Authentication key\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5468b5da",
   "metadata": {},
   "source": [
    "## Step 6: LM Studio Configuration\n",
    "\n",
    "LM Studio provides a local AI inference server that's compatible with OpenAI's API. This allows you to run AI models locally on your machine without sending data to external services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c9f40d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For LM Studio, we can use the OpenAI interface directly\n",
    "# Note: LM Studio runs on localhost:1234 by default, so we don't need an API key\n",
    "# If you have LM Studio installed, you can use it as follows:\n",
    "from langchain_openai import ChatOpenAI  # Import OpenAI interface for LM Studio\n",
    "\n",
    "# Initialize LM Studio chat model (replace with your model path or name)\n",
    "# Initialize LM Studio chat model (LM Studio runs on localhost:1234 by default)\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"not-needed\"  # LM Studio doesn't require an API key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b4b99",
   "metadata": {},
   "source": [
    "## Step 7: Making Your First LLM Call\n",
    "\n",
    "The `invoke()` method is the primary way to send prompts to language models in LangChain. It returns a response object containing the AI's answer along with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a3523b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"What is the capital of India?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede1f595",
   "metadata": {},
   "source": [
    "## Step 8: Understanding Response Objects\n",
    "\n",
    "LangChain returns structured response objects that contain both the AI's answer and additional metadata like model information, token usage, and processing details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f83adb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Simple question. Capital of India is New Delhi.The capital city of India is **New\\u202fDelhi**.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 74, 'total_tokens': 95, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'openai/gpt-oss-20b', 'id': 'chatcmpl-hkkhbxyu36rdoaoe29rgb8', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--2c1afebf-b00b-4f67-a9bb-cdfa2708e0fa-0' usage_metadata={'input_tokens': 74, 'output_tokens': 21, 'total_tokens': 95, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc24b6e",
   "metadata": {},
   "source": [
    "## Step 9: Extracting Response Content\n",
    "\n",
    "For most applications, you'll want just the text content of the AI's response. The `.content` attribute provides this without the metadata wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1eb052be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple question. Capital of India is New Delhi.The capital city of India is **New Delhi**.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)  # Print the content of the response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb07c58",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully completed the LangChain basics! You've learned:\n",
    "\n",
    "✅ **Environment Setup**: Created isolated Python environment  \n",
    "✅ **Dependency Installation**: Installed LangChain and related packages  \n",
    "✅ **Local AI Configuration**: Connected to LM Studio for local inference  \n",
    "✅ **Basic LLM Calls**: Made your first AI model invocation  \n",
    "✅ **Response Handling**: Understood response objects and content extraction  \n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **LangChain**: Framework for building applications with language models\n",
    "- **LM Studio**: Local AI inference server with OpenAI-compatible API\n",
    "- **invoke()**: Primary method for sending prompts to LLMs\n",
    "- **Response Objects**: Structured containers for AI responses and metadata\n",
    "- **Local vs Cloud**: Benefits of running AI models locally (privacy, cost, control)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebooks, you'll explore:\n",
    "- **Advanced Prompting**: Structured prompts and templates\n",
    "- **Chains**: Connecting multiple AI operations together\n",
    "- **RAG**: Retrieval-Augmented Generation for context-aware AI\n",
    "- **Vector Databases**: Semantic search and embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
