{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8f259d",
   "metadata": {},
   "source": [
    "# LangChain Context-Aware AI: Introduction to RAG\n",
    "\n",
    "## Overview\n",
    "This notebook introduces context-aware AI and the fundamentals of RAG (Retrieval-Augmented Generation). You'll learn how to provide specific context to AI models to get more accurate and reliable responses.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand context-aware prompting and its benefits\n",
    "- Learn RAG (Retrieval-Augmented Generation) principles\n",
    "- Build prompts that use external context effectively\n",
    "- Handle cases where context doesn't contain requested information\n",
    "- Prevent AI hallucination with context-based constraints\n",
    "\n",
    "## Key Concepts\n",
    "- **Context-Aware AI**: Providing specific information to guide AI responses\n",
    "- **RAG**: Retrieval-Augmented Generation for grounded AI answers\n",
    "- **Context Constraints**: Limiting AI responses to provided information only\n",
    "- **Fallback Responses**: Handling insufficient context gracefully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b24ed",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "This notebook provides the same two configuration options as previous notebooks:\n",
    "- **Azure OpenAI** (commented out) for cloud-based AI\n",
    "- **LM Studio** (active) for local AI inference\n",
    "\n",
    "We'll continue using LM Studio for local development and learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13428d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with Azure OpenAI and LangChain\n",
    "\n",
    "# from langchain_openai import AzureChatOpenAI           # Azure OpenAI integration\n",
    "# from langchain_core.prompts import ChatPromptTemplate  # For structured prompts\n",
    "# from langchain_core.output_parsers import StrOutputParser  # For cleaning outputs\n",
    "\n",
    "# # Message types for conversation structure\n",
    "# from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# # Environment and configuration\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# # Initialize Azure OpenAI client with credentials\n",
    "# llm = AzureChatOpenAI(\n",
    "#     azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),          \n",
    "#     azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), \n",
    "#     api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),          \n",
    "#     api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),                  \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d1055",
   "metadata": {},
   "source": [
    "### Option 1: Azure OpenAI Configuration (Commented Out)\n",
    "Cloud-based AI configuration for production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6aebdd",
   "metadata": {},
   "source": [
    "### Option 2: LM Studio Configuration (Active)\n",
    "Local AI configuration for development and learning. Ensure LM Studio is running with a model loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a49722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with local LM Studio and LangChain\n",
    "\n",
    "from langchain_openai import ChatOpenAI           # Azure OpenAI integration\n",
    "from langchain_core.prompts import ChatPromptTemplate  # For structured prompts\n",
    "from langchain_core.output_parsers import StrOutputParser  # For cleaning outputs\n",
    "\n",
    "# Message types for conversation structure\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Environment and configuration\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LM Studio client\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"not-needed\"  # LM Studio doesn't require an API key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5569e",
   "metadata": {},
   "source": [
    "## Step 2: Building a RAG-Ready Prompt Template\n",
    "\n",
    "This prompt template is designed for RAG (Retrieval-Augmented Generation) workflows. It includes:\n",
    "\n",
    "### Key RAG Features:\n",
    "- **Context Integration**: Dedicated space for external information\n",
    "- **Strict Constraints**: AI must use only provided context\n",
    "- **Fallback Handling**: Clear response when context is insufficient\n",
    "- **Hallucination Prevention**: Prevents AI from making up information\n",
    "\n",
    "### Template Structure:\n",
    "- **System Instructions**: Define AI behavior and constraints\n",
    "- **Context Section**: Where retrieved information goes\n",
    "- **Question Section**: User's actual query\n",
    "- **Reminder**: Reinforces context-only responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "165f1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"you are a helpful and comic assistant that help users to find the answers in fun way based on given context.\n",
    "       \n",
    "       IMPORTANT RAG INSTRUCTIONS:\n",
    "       - Use ONLY the information provided in the context below\n",
    "       - If the context doesn't contain enough information to answer the question, say: \"I don't have enough info on this :(\"\n",
    "       - Do not make up information or use knowledge outside the provided context\n",
    "       - Be helpful and engaging while staying factually accurate\n",
    "       \n",
    "       ----\n",
    "       Context: {context}\n",
    "       ----\n",
    "       Question: {question}\n",
    "       \n",
    "       Remember: Base your answer ONLY on the context provided above!\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596eb7af",
   "metadata": {},
   "source": [
    "## Step 3: Creating the Processing Chain\n",
    "\n",
    "Building our familiar three-component chain for clean, processed output:\n",
    "`prompt_template | llm | parser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814aecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()\n",
    "chain = prompt_template | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf60170c",
   "metadata": {},
   "source": [
    "## Step 4: Setting Up Context Data\n",
    "\n",
    "In real RAG systems, this context would come from:\n",
    "- **Vector databases** (similarity search)\n",
    "- **Document retrieval** (relevant text chunks)\n",
    "- **Knowledge bases** (structured information)\n",
    "\n",
    "For learning purposes, we'll use a simple string with basic moon facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce1fe53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_data = \"moon is earth's natural satelite and orbit around it in 28 days cycle\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc978c3b",
   "metadata": {},
   "source": [
    "## Step 5: Testing Context-Aware Responses\n",
    "\n",
    "Let's test our RAG system with three different scenarios:\n",
    "\n",
    "### Test 1: Information Available in Context\n",
    "Asking about the moon - our context contains this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcdb00e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, buckle up for some lunar knowledge! 🌕\n",
      "\n",
      "According to my super-secret intel (aka the context you gave me!), the moon is Earth's natural satellite! It zooms around our planet in a cycle that takes about 28 days. ✨\n",
      "\n",
      "Pretty neat, huh? Though I gotta say, my info is a *little* limited... I don't have all the details on what it's made of or if there are little green cheese people living there! 🧀👽\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"what is moon\"\n",
    "response = chain.invoke({\"context\": context_data, \"question\": question})\n",
    "print(response)  # Print the content of the response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30181fbc",
   "metadata": {},
   "source": [
    "### Test 2: Partial Information in Context\n",
    "Asking about Earth - our context mentions Earth but doesn't fully describe it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79a5be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, buckle up for some Earth-shattering (not literally, we're keeping it chill!) info! \n",
      "\n",
      "According to my super-secret documents (aka the context you gave me!), Earth has a natural satellite called... the Moon! And that moon orbits around it. \n",
      "\n",
      "So, Earth is a planet that has a moon orbiting around it in a 28-day cycle. Pretty neat, huh? ✨🌙\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"what is earth\"\n",
    "response = chain.invoke({\"context\": context_data, \"question\": question})\n",
    "print(response)  # Print the content of the response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e22cbb6",
   "metadata": {},
   "source": [
    "### Test 3: Information NOT in Context\n",
    "Asking about the sun - our context contains no information about the sun. This tests our fallback response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bdaa257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, that's a tricky one! Based on what I know (which is just this little bit of info!), the context doesn't say anything about where the Sun is. It only talks about the Moon and Earth! \n",
      "\n",
      "I don't have enough info on this :(. Maybe ask me something about our lunar friend? I'm a Moon expert, apparently! 😉\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"where is sun?\"\n",
    "response = chain.invoke({\"context\": context_data, \"question\": question})\n",
    "print(response)  # Print the content of the response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36abc0c6",
   "metadata": {},
   "source": [
    "## Step 6: Analyzing RAG Behavior\n",
    "\n",
    "### What We Observed:\n",
    "\n",
    "**✅ Context Available**: AI provides accurate answer based on provided information  \n",
    "**⚠️ Partial Context**: AI works with limited information or indicates insufficient data  \n",
    "**❌ No Context**: AI properly responds with \"I don't have enough info\" instead of hallucinating  \n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "- **Accuracy**: Responses are grounded in actual data\n",
    "- **Reliability**: No false information from AI hallucination\n",
    "- **Transparency**: Clear when information is insufficient\n",
    "- **Trust**: Users know responses are fact-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34a4b96",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully learned the fundamentals of context-aware AI and RAG! You've mastered:\n",
    "\n",
    "✅ **RAG Prompt Design**: Created prompts that integrate external context  \n",
    "✅ **Context Constraints**: Limited AI responses to provided information only  \n",
    "✅ **Fallback Handling**: Managed cases with insufficient context gracefully  \n",
    "✅ **Hallucination Prevention**: Stopped AI from making up information  \n",
    "✅ **Reliable AI**: Built trustworthy AI responses grounded in facts  \n",
    "\n",
    "## Key Concepts Learned\n",
    "\n",
    "- **RAG (Retrieval-Augmented Generation)**: Enhancing AI with external context\n",
    "- **Context Integration**: Providing specific information to guide AI responses\n",
    "- **Strict Constraints**: Using \"ONLY\" instructions to prevent hallucination\n",
    "- **Fallback Responses**: Graceful handling when context is insufficient\n",
    "- **Grounded AI**: AI responses based on factual information\n",
    "\n",
    "## RAG Benefits\n",
    "\n",
    "- **Accuracy**: Responses based on actual data, not general training\n",
    "- **Up-to-date**: Can include recent information not in training data\n",
    "- **Domain-specific**: Tailored responses for specific knowledge areas\n",
    "- **Trustworthy**: Clear source of information for responses\n",
    "- **Controllable**: Predictable behavior within context boundaries\n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "- **Customer Support**: Answer questions using company documentation\n",
    "- **Research Assistant**: Provide answers from specific research papers\n",
    "- **Legal AI**: Respond based on specific case law and regulations\n",
    "- **Medical AI**: Answer questions using current medical literature\n",
    "- **Education**: Teach concepts using specific curriculum materials\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, you'll explore:\n",
    "- **Embeddings**: Converting text to numerical vectors for similarity search\n",
    "- **Vector Similarity**: Finding relevant context automatically\n",
    "- **Advanced RAG**: Building smarter context retrieval systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
