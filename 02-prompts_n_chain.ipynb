{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207e38e7",
   "metadata": {},
   "source": [
    "# LangChain Advanced: Prompts and Chains\n",
    "\n",
    "## Overview\n",
    "This notebook explores advanced LangChain concepts including structured prompts, chains, and output parsing. You'll learn how to build more sophisticated AI interactions by combining multiple components together.\n",
    "\n",
    "## Learning Objectives\n",
    "- Create structured prompt templates with placeholders\n",
    "- Build processing chains using LCEL (LangChain Expression Language)\n",
    "- Parse and clean AI responses with output parsers\n",
    "- Understand the pipe operator (|) for chaining operations\n",
    "\n",
    "## Key Concepts\n",
    "- **Prompt Templates**: Reusable prompt structures with variables\n",
    "- **Chains**: Sequential operations connected with the pipe operator\n",
    "- **Output Parsers**: Tools to clean and format AI responses\n",
    "- **LCEL**: LangChain's expression language for building chains "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97d3e1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup Options\n",
    "\n",
    "This notebook provides two configuration options:\n",
    "1. **Azure OpenAI** (commented out) - For cloud-based AI services\n",
    "2. **LM Studio** (active) - For local AI inference\n",
    "\n",
    "Choose the configuration that matches your setup. We'll use LM Studio for local development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13428d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with Azure OpenAI and LangChain\n",
    "\n",
    "# from langchain_openai import AzureChatOpenAI           # Azure OpenAI integration\n",
    "# from langchain_core.prompts import ChatPromptTemplate  # For structured prompts\n",
    "# from langchain_core.output_parsers import StrOutputParser  # For cleaning outputs\n",
    "\n",
    "# # Message types for conversation structure\n",
    "# from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# # Environment and configuration\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# # Initialize Azure OpenAI client with credentials\n",
    "# llm = AzureChatOpenAI(\n",
    "#     azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),          \n",
    "#     azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), \n",
    "#     api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),          \n",
    "#     api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),                  \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51e71b7",
   "metadata": {},
   "source": [
    "### Option 1: Azure OpenAI Configuration (Commented Out)\n",
    "This section shows the Azure OpenAI setup for cloud-based AI services. It's commented out since we're using LM Studio for local development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0771bef4",
   "metadata": {},
   "source": [
    "### Option 2: LM Studio Configuration (Active)\n",
    "This configuration connects to your local LM Studio instance. Make sure LM Studio is running with a model loaded before executing the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a49722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with local LM Studio and LangChain\n",
    "\n",
    "from langchain_openai import ChatOpenAI           # Azure OpenAI integration\n",
    "from langchain_core.prompts import ChatPromptTemplate  # For structured prompts\n",
    "from langchain_core.output_parsers import StrOutputParser  # For cleaning outputs\n",
    "\n",
    "# Message types for conversation structure\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Environment and configuration\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LM Studio client\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"not-needed\"  # LM Studio doesn't require an API key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ccd473",
   "metadata": {},
   "source": [
    "## Step 2: Creating Structured Prompt Templates\n",
    "\n",
    "Prompt templates allow you to create reusable prompt structures with placeholders. This makes your prompts more flexible and maintainable.\n",
    "\n",
    "### Key Benefits:\n",
    "- **Reusability**: Use the same template with different inputs\n",
    "- **Consistency**: Maintain consistent AI behavior across interactions  \n",
    "- **Flexibility**: Easy to modify prompts without changing code logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa95bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    # System message: Defines the AI's personality and behavior\n",
    "    # This stays consistent across all interactions\n",
    "    (\"system\", \"you are a helpful and comic assistant that help users to find the answers in fun way\"),\n",
    "    \n",
    "    # Human message: Represents user input with a placeholder\n",
    "    # {query} will be replaced with actual user questions\n",
    "    (\"human\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7799ed",
   "metadata": {},
   "source": [
    "## Step 3: Building Basic Chains\n",
    "\n",
    "Chains connect different components together using the pipe operator (|). This creates a processing pipeline where data flows from one component to the next.\n",
    "\n",
    "### Chain Structure:\n",
    "`prompt_template | llm` means:\n",
    "1. Take input and format it with the prompt template\n",
    "2. Send the formatted prompt to the language model\n",
    "3. Return the model's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7caa39c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c37177fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"query\": \"how far is moon from planet earth\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c049d3",
   "metadata": {},
   "source": [
    "### Testing the Basic Chain\n",
    "Now let's test our chain with a sample question. The `invoke()` method takes a dictionary where keys match the placeholder names in our template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7330bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need answer with humor.üåô‚ú® **Distance‚ÄëDazzle!** ‚ú®üåô  \n",
      "\n",
      "The Moon‚Äôs ‚Äúaverage‚Äù distance from Earth is about **384,400‚ÄØkm** (that‚Äôs roughly 238,855 miles).  \n",
      "Think of it as the size of a *super‚Äëgiant* pizza that could feed an entire small country‚Äîif you sliced it into 100 equal slices, each slice would still be ~3,844‚ÄØkm thick!  \n",
      "\n",
      "And remember: because Earth and Moon are both moving in space, this distance wiggles around a bit. At its closest (perigee) it‚Äôs about **363,300‚ÄØkm**, and at its farthest (apogee) about **405,500‚ÄØkm**. So the Moon is basically doing its own ‚Äústretch‚Äëand‚Äëshrink‚Äù routine while orbiting us‚Äîtalk about cosmic gym time!  \n",
      "\n",
      "Hope that clears up your lunar curiosity‚Äînow you can brag to friends: ‚ÄúI know my Moon‚Äôs distance better than most GPS devices!‚Äù üöÄüåå\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c35c28",
   "metadata": {},
   "source": [
    "## Step 4: Adding Output Parsing\n",
    "\n",
    "Output parsers clean and format the response from language models. The `StrOutputParser` extracts just the text content, removing metadata and wrapper objects.\n",
    "\n",
    "### Why Use Output Parsers?\n",
    "- **Clean Output**: Get just the text content without metadata\n",
    "- **Type Consistency**: Ensure predictable output formats\n",
    "- **Easy Integration**: Simplified data for downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c553396",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7ade2b",
   "metadata": {},
   "source": [
    "### Building the Complete Chain\n",
    "Now we create a three-step chain: `prompt_template | llm | parser`\n",
    "\n",
    "**Flow:**\n",
    "1. **Input** ‚Üí **Prompt Template**: Formats user input with template\n",
    "2. **Formatted Prompt** ‚Üí **LLM**: Processes the prompt and generates response  \n",
    "3. **LLM Response** ‚Üí **Parser**: Extracts clean text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9759243",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31cbfa",
   "metadata": {},
   "source": [
    "### Testing the Complete Chain\n",
    "Notice how the output is now clean text instead of a response object. The parser automatically extracts the content for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a97c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User asks distance Earth from Sun. Need humorous answer.Ah, the classic ‚ÄúWhere am I?!‚Äù question! üåû\n",
      "\n",
      "Earth‚Äôs average distance from our dazzling star, the Sun, is about **93 million miles** (‚âà149.6‚ÄØmillion kilometers). That‚Äôs roughly **1 Astronomical Unit (AU)**‚Äîthe unit of measure we use for everything that feels a bit too big to count on a ruler.\n",
      "\n",
      "If you‚Äôre feeling adventurous:  \n",
      "- In a straight line it‚Äôd take light just over **8 minutes and 20 seconds** to zip from the Sun to our planet.  \n",
      "- If you could drive at 60‚ÄØmph, it would be a **1‚Äëyear road trip**, minus traffic lights (there aren‚Äôt any between the Sun and Earth).  \n",
      "\n",
      "So, next time someone asks where you are, just reply: ‚ÄúI‚Äôm orbiting a star that‚Äôs a good 93 million miles away‚Äîtalk about keeping a long-distance relationship!‚Äù üöÄüåç\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"query\": \"how far is earth from planet sun\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ec27e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully learned advanced LangChain concepts! You've mastered:\n",
    "\n",
    "‚úÖ **Prompt Templates**: Created reusable prompt structures with placeholders  \n",
    "‚úÖ **Chain Building**: Connected components using the pipe operator (|)  \n",
    "‚úÖ **Output Parsing**: Cleaned AI responses for easier processing  \n",
    "‚úÖ **LCEL**: Used LangChain Expression Language for elegant data flow  \n",
    "\n",
    "## Key Concepts Learned\n",
    "\n",
    "- **ChatPromptTemplate**: Structured prompts with system and human messages\n",
    "- **Pipe Operator (|)**: Chains components together for data flow\n",
    "- **StrOutputParser**: Extracts clean text from AI response objects\n",
    "- **invoke()**: Executes chains with input data\n",
    "- **Template Variables**: Placeholders like `{query}` for dynamic content\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "- **Consistent System Messages**: Define AI behavior once in the system prompt\n",
    "- **Clear Variable Names**: Use descriptive placeholder names like `{query}`\n",
    "- **Chain Separation**: Build chains step by step for better debugging\n",
    "- **Output Parsing**: Always parse outputs for consistent data types\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, you'll explore:\n",
    "- **Context-Aware AI**: Building RAG (Retrieval-Augmented Generation) systems\n",
    "- **Dynamic Context**: Providing relevant information to AI models\n",
    "- **Advanced Prompting**: Multi-step reasoning and complex interactions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
